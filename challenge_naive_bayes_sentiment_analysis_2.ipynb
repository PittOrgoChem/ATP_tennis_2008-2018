{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHALLENGE: Sentiment Analysis and Naive Bayes (Part 2)\n",
    "\n",
    "## By Jean-Philippe Pitteloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working dataset was selected from data available on the University of California Irvine Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). A group of files were manually downloaded and the \"Yelp\" (yelp_labelled.txt) dataset for the model was selected responding to personal interests. The data from the downloaded file was read as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_raw = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_raw.columns = ['sentence', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>I LOVED their mussels cooked in this wine redu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>They have a really nice atmosphere.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>If you want to wait for mediocre food and down...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Not to mention the combination of pears, almon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>Probably not in a hurry to go back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Great place to eat, reminds me of the little m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The only redeeming quality of the restaurant w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>I as well would've given godfathers zero stars...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>We made the drive all the way from North Scott...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  score\n",
       "103  I LOVED their mussels cooked in this wine redu...      1\n",
       "716                They have a really nice atmosphere.      1\n",
       "931  If you want to wait for mediocre food and down...      0\n",
       "445  Not to mention the combination of pears, almon...      1\n",
       "941                Probably not in a hurry to go back.      0\n",
       "403  Great place to eat, reminds me of the little m...      1\n",
       "35   The only redeeming quality of the restaurant w...      1\n",
       "257  I as well would've given godfathers zero stars...      0\n",
       "608  We made the drive all the way from North Scott...      1\n",
       "21                                  The food, amazing.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_raw.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responding to our interest of creating a model to evaluate sentiments in text and produce a score in terms of \"positive\" or \"negative\" sentiment, the first step was to simplify the format of the text available by removing special characters and turning every letter in the text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_raw['sentence'] = yelp_raw['sentence'].str.lower().str.replace(r'[,.!]+', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of which words are more commonly found in both \"positive\" and \"negative\" comments/reviews, all comments in every group of reviews were splitted in their composing words and the count of words was summarized from most common to least common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_words = pd.Series(' '.join(yelp_raw[yelp_raw['score'] == 1]['sentence']).lower().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = pd.Series(' '.join(yelp_raw[yelp_raw['score'] == 0]['sentence']).lower().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in Positive comments:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             698\n",
       "the          310\n",
       "and          222\n",
       "was          138\n",
       "i            117\n",
       "a            112\n",
       "is           104\n",
       "to            87\n",
       "this          77\n",
       "good          73\n",
       "great         70\n",
       "food          60\n",
       "in            59\n",
       "place         57\n",
       "of            53\n",
       "it            51\n",
       "very          47\n",
       "service       45\n",
       "for           43\n",
       "with          42\n",
       "had           37\n",
       "are           36\n",
       "so            35\n",
       "you           34\n",
       "we            34\n",
       "were          34\n",
       "have          33\n",
       "my            33\n",
       "on            32\n",
       "they          32\n",
       "here          29\n",
       "all           25\n",
       "friendly      24\n",
       "that          24\n",
       "back          23\n",
       "delicious     23\n",
       "be            23\n",
       "our           22\n",
       "time          22\n",
       "nice          22\n",
       "really        22\n",
       "best          22\n",
       "amazing       21\n",
       "but           20\n",
       "their         19\n",
       "just          18\n",
       "as            18\n",
       "also          18\n",
       "not           18\n",
       "an            17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Most common words in Positive comments:\\n')\n",
    "pos_words.value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in Negative comments:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "           674\n",
       "the        274\n",
       "i          187\n",
       "and        169\n",
       "was        157\n",
       "to         131\n",
       "a          125\n",
       "not         98\n",
       "it          82\n",
       "of          74\n",
       "is          67\n",
       "for         67\n",
       "this        66\n",
       "food        65\n",
       "place       49\n",
       "in          48\n",
       "we          45\n",
       "be          44\n",
       "that        43\n",
       "but         42\n",
       "at          40\n",
       "my          39\n",
       "back        38\n",
       "service     37\n",
       "had         33\n",
       "so          31\n",
       "with        30\n",
       "were        29\n",
       "have        29\n",
       "like        29\n",
       "very        29\n",
       "here        28\n",
       "there       28\n",
       "are         26\n",
       "go          26\n",
       "you         25\n",
       "they        24\n",
       "on          23\n",
       "no          23\n",
       "good        22\n",
       "never       22\n",
       "will        22\n",
       "don't       22\n",
       "would       21\n",
       "time        20\n",
       "if          20\n",
       "our         19\n",
       "ever        19\n",
       "minutes     19\n",
       "as          18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Most common words in Negative comments:\\n')\n",
    "neg_words.value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the lists of words displayed above, a list of keywords associated to both positive and negative comments was created and new features in our working dataset, indicating the presence or absence of the keywords in a given comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_1 = yelp_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_keywords = ['good', 'great', 'friendly', 'delicious', 'nice', 'best', 'amazing', 'like', 'love', 'fantastic', 'awesome', 'pretty', 'loved', 'excellent', 'tasty', 'recommend', 'fresh', 'not', 'bad', 'terrible', 'worst', 'disgusting', 'never', 'won\"t', 'dissapointed', 'dissapointing']\n",
    "\n",
    "for word in pos_keywords:\n",
    "    yelp_1[str(word)] = yelp_1['sentence'].str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working dataframe was splitted into two new dataframes. The 'data' dataset contained all new features created associated to the selecte keywords, while the 'target' dataset contain only the values associated to the score received by the original comment in terms of \"positive\" or \"negative\" sentiment associated to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yelp_1[pos_keywords]\n",
    "target = yelp_1['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, the necessary requirements were imported to apply a Naive Bayes Bernoulli classification model and the model executed using the two new datasets created above ('data' and 'target'). Once the model was built, the model was used to predict scores/values and its performance compared to the scores assigned in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 242\n"
     ]
    }
   ],
   "source": [
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "model_bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "model_bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = model_bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], (target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea about the performance of out first classifier, a confusion matrix was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[463,  37],\n",
       "       [205, 295]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mod1_conf_mat = confusion_matrix(target, y_pred)\n",
    "mod1_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information provided by the confusion matrix, the accuracy of the classifier was calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.8\n"
     ]
    }
   ],
   "source": [
    "accuracy = (mod1_conf_mat[0,0] + mod1_conf_mat[1,1]) * 100 / data.shape[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand the scope of our analysis, the built-in method 'classification_report' was used and the results displayed below. The report summarizes typically used metrics to evaluate the performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.93      0.79       500\n",
      "           1       0.89      0.59      0.71       500\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1000\n",
      "   macro avg       0.79      0.76      0.75      1000\n",
      "weighted avg       0.79      0.76      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(target, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, cross validation was performed to evaluate the tendency of the model to OVERFIT the data based on the data, conditions, and parameter used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.74, 0.72, 0.78, 0.8 , 0.72, 0.78, 0.82, 0.76, 0.78, 0.72,\n",
       "       0.66, 0.86, 0.76, 0.7 , 0.8 , 0.74, 0.76, 0.78, 0.72])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model_bnb, data, target, cv = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the results of our analysis about the performance of this version of our sentiment classifier reflect that the model (model #1) has some degree of overfittin based on the variation in the score during the cross_validation process. Considering the training data is also the testing data, overfitting is very likely. Also, the high number of features including in the training phase may also impact the generality of the model. In terms of performance of the model, the selection of words/features to identify 'positive' sentiments presents good precision (89%) while performs poorly classifying negative sentiments (69%). In terms of recall, a large number of negative sentiment driven comments was correctly predicted by our model (93%) while a only 59% of positive sentiment driven comments was correctly predicted. The combined F-1 score also confirmed a better performance of our model while predicting negative sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, variations of the models were evaluated using the same metrics used above, and some conclusions driven from the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_2 = yelp_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular model, words with negative meaning were removed from the list of words/features present in the original model and the performance evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words related to negative experiences removed\n",
    "pos_keywords_2 = ['good', 'great', 'friendly', 'delicious', 'nice', 'best', 'amazing', 'like', 'love', 'fantastic', 'awesome', 'pretty', 'loved', 'excellent', 'tasty', 'recommend', 'fresh']\n",
    "\n",
    "for word in pos_keywords_2:\n",
    "    yelp_2[str(word)] = yelp_2['sentence'].str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = yelp_2[pos_keywords_2]\n",
    "target_2 = yelp_2['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnb_2 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "model_bnb_2.fit(data_2, target_2)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred_2 = model_bnb_2.predict(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[450,  50],\n",
       "       [213, 287]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_2_conf_mat = confusion_matrix(target_2, y_pred_2)\n",
    "mod_2_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.7\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = (mod_2_conf_mat[0,0] + mod_2_conf_mat[1,1]) * 100 / data_2.shape[0]\n",
    "\n",
    "print('Accuracy:', accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the removal of negative sentiment words from the features reduced the accuracy of the model from the original 76%. Also, all metrics reported below were also reduced to a certain extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.77       500\n",
      "           1       0.85      0.57      0.69       500\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1000\n",
      "   macro avg       0.77      0.74      0.73      1000\n",
      "weighted avg       0.77      0.74      0.73      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6 , 0.74, 0.76, 0.76, 0.8 , 0.72, 0.78, 0.82, 0.66, 0.76, 0.72,\n",
       "       0.66, 0.8 , 0.68, 0.7 , 0.76, 0.7 , 0.72, 0.8 , 0.74])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_bnb_2, data_2, target_2, cv=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overfitting, the results from cross-validation suggest some degree of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_3 = yelp_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, words/features related to food were not employed in order to gain generality in the model and reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like the original but using less words related to food\n",
    "pos_keywords_3 = ['good', 'great', 'friendly', 'nice', 'best', 'amazing', 'like', 'love', 'fantastic', 'awesome', 'pretty', 'loved', 'excellent', 'recommend', 'not', 'bad', 'terrible', 'worst', 'never', 'won\"t', 'dissapointed', 'dissapointing']\n",
    "\n",
    "for word in pos_keywords_3:\n",
    "    yelp_3[str(word)] = yelp_3['sentence'].str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = yelp_3[pos_keywords_3]\n",
    "target_3 = yelp_3['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnb_3 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "model_bnb_3.fit(data_3, target_3)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred_3 = model_bnb_3.predict(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[467,  33],\n",
       "       [234, 266]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_3_conf_mat = confusion_matrix(target_3, y_pred_3)\n",
    "mod_3_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.3\n"
     ]
    }
   ],
   "source": [
    "accuracy_3 = (mod_3_conf_mat[0,0] + mod_3_conf_mat[1,1]) * 100 / data_3.shape[0]\n",
    "\n",
    "print('Accuracy:', accuracy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the changes introduced in this version of the model compared to the original, caused a decrease in the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.93      0.78       500\n",
      "           1       0.89      0.53      0.67       500\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1000\n",
      "   macro avg       0.78      0.73      0.72      1000\n",
      "weighted avg       0.78      0.73      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of other key metrics, also reflect a decrease in the ability of this model to correctly predict the sentiment of the comments present in the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68, 0.68, 0.74, 0.76, 0.74, 0.68, 0.72, 0.74, 0.76, 0.72, 0.68,\n",
       "       0.64, 0.8 , 0.76, 0.72, 0.8 , 0.72, 0.76, 0.74, 0.74])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_bnb_3, data_3, target_3, cv=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overfitting, cross-validation results still show fluctuations that suggest overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_4 = yelp_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of our classifier, the number of words/features was drastically reduced in an attempt to create a simpler and more general model less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like the original reducing the list of words\n",
    "pos_keywords_4 = ['good', 'great', 'nice', 'like', 'excellent', 'not', 'bad']\n",
    "\n",
    "for word in pos_keywords_4:\n",
    "    yelp_4[str(word)] = yelp_4['sentence'].str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = yelp_4[pos_keywords_4]\n",
    "target_4 = yelp_4['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnb_4 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "model_bnb_4.fit(data_4, target_4)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred_4 = model_bnb_4.predict(data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[489,  11],\n",
       "       [342, 158]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_4_conf_mat = confusion_matrix(target_4, y_pred_4)\n",
    "mod_4_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.7\n"
     ]
    }
   ],
   "source": [
    "accuracy_4 = (mod_4_conf_mat[0,0] + mod_4_conf_mat[1,1]) * 100 / data_4.shape[0]\n",
    "\n",
    "print('Accuracy:', accuracy_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the accuracy of this last model in predicting the sentiment of the comments present in the testing dataset was dramatically reduced in comparison to the original model (76%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.98      0.73       500\n",
      "           1       0.93      0.32      0.47       500\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1000\n",
      "   macro avg       0.76      0.65      0.60      1000\n",
      "weighted avg       0.76      0.65      0.60      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_4, y_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other key metrics also were severely impacted making the new model worse than the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62, 0.66, 0.66, 0.66, 0.62, 0.62, 0.68, 0.66, 0.64, 0.6 , 0.6 ,\n",
       "       0.6 , 0.7 , 0.6 , 0.68, 0.66, 0.64, 0.7 , 0.68, 0.66])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_bnb_4, data_4, target_4, cv=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation process, suggest this model to be less prone to overfit the data. The use of less features and more general words may be responsible for the seen decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_5 = yelp_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last version of the model, all negative sentiment words were removed and new features introduced that use the negative version (not_) of each of the positive words included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like the original list but negative words removed and new features about not_word created instead\n",
    "pos_keywords_5 = ['good', 'great', 'friendly', 'delicious', 'nice', 'best', 'amazing', 'like', 'love', 'fantastic', 'awesome', 'pretty', 'loved', 'excellent', 'tasty', 'recommend', 'fresh']\n",
    "\n",
    "for word in pos_keywords_5:\n",
    "    yelp_5[str(word)] = yelp_5['sentence'].str.contains(str(word), case=False)\n",
    "    yelp_5['not_' + str(word)] = (yelp_5['sentence'].str.contains('not', case=False) & yelp_5['sentence'].str.contains(str(word), case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = yelp_5.iloc[:, 2:]\n",
    "target_5 = yelp_5['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnb_5 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "model_bnb_5.fit(data_5, target_5)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred_5 = model_bnb_5.predict(data_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[467,  33],\n",
       "       [216, 284]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_5_conf_mat = confusion_matrix(target_5, y_pred_5)\n",
    "mod_5_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen below, this version performs very similar to the original model in terms of accuracy (75% vs 76%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.1\n"
     ]
    }
   ],
   "source": [
    "accuracy_5 = (mod_5_conf_mat[0,0] + mod_5_conf_mat[1,1]) * 100 / data_5.shape[0]\n",
    "\n",
    "print('Accuracy:', accuracy_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of other key metrics, this version outperforms the original model slightly in some metrics while performs slightly worse in others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.93      0.79       500\n",
      "           1       0.90      0.57      0.70       500\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1000\n",
      "   macro avg       0.79      0.75      0.74      1000\n",
      "weighted avg       0.79      0.75      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_5, y_pred_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66, 0.76, 0.74, 0.78, 0.82, 0.72, 0.76, 0.82, 0.66, 0.76, 0.74,\n",
       "       0.66, 0.84, 0.74, 0.7 , 0.76, 0.72, 0.76, 0.8 , 0.74])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_bnb_5, data_5, target_5, cv = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as overfitting goes, this version exhibits similar fluctuations than the ones observed for the original model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
